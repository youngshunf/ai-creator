# AI Creator - Agent Runtime

> ç«¯äº‘ç»Ÿä¸€ Agent Runtime æŠ½è±¡å±‚ï¼ˆä¸‰å±‚æ¶æ„è®¾è®¡ï¼‰

## 1. è®¾è®¡ç†å¿µ

### 1.1 æ ¸å¿ƒç›®æ ‡

**Graph/Agent ä»£ç "å”¯ä¸€æ¥æº"ï¼Œç«¯äº‘ç»Ÿä¸€æ‰§è¡Œ**

- **å•ä¸€æ¥æº**: Graph å®šä¹‰æ–‡ä»¶ç‰ˆæœ¬æ§åˆ¶ï¼Œç«¯ä¾§å’Œäº‘ç«¯å…±äº«åŒä¸€ä»½ä»£ç 
- **ç«¯äº‘å¯¹ç­‰**: ç›¸åŒçš„ Graph åœ¨æœ¬åœ°æˆ–äº‘ç«¯æ‰§è¡Œï¼Œè¡Œä¸ºä¸€è‡´
- **å·¥å…·éš”ç¦»**: å·¥å…·å±‚å±è”½å¹³å°å·®å¼‚ï¼Œé€šè¿‡èƒ½åŠ›å£°æ˜å’Œé™çº§è·¯å¾„å¤„ç†
- **èµ„æºç»Ÿä¸€**: ç»Ÿä¸€èµ„æº URI æ–¹æ¡ˆï¼Œé¿å…ç¡¬ç¼–ç è·¯å¾„

### 1.2 ä¸‰å±‚æ¶æ„æ¦‚è§ˆ

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Agent Runtime ä¸‰å±‚æ¶æ„                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Layer 1: Agent Definition Layer                          â”‚  â”‚
â”‚  â”‚                     (Graph å®šä¹‰å±‚)                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  YAML/JSON å£°æ˜å¼ Graph å®šä¹‰ (å”¯ä¸€æ¥æºï¼Œç‰ˆæœ¬æ§åˆ¶)                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                                 â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ agent-definitions/                                         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   â”œâ”€â”€ content-creation.yaml    # å†…å®¹åˆ›ä½œ Graph              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   â”œâ”€â”€ publish-workflow.yaml    # å‘å¸ƒå·¥ä½œæµ Graph            â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   â”œâ”€â”€ analytics.yaml           # æ•°æ®åˆ†æ Graph              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚   â””â”€â”€ topic-recommend.yaml     # é€‰é¢˜æ¨è Graph              â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                      â”‚
â”‚                                      â”‚ GraphLoader.load()                   â”‚
â”‚                                      â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Layer 2: Agent Runtime Layer                             â”‚  â”‚
â”‚  â”‚                    (æ‰§è¡Œå™¨å±‚)                                          â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚       â”‚  LocalExecutor  â”‚              â”‚  CloudExecutor  â”‚           â”‚  â”‚
â”‚  â”‚       â”‚  (æ¡Œé¢ç«¯/Python) â”‚              â”‚  (äº‘ç«¯/FastAPI) â”‚           â”‚  â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚                â”‚                                â”‚                    â”‚  â”‚
â”‚  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â”‚                             â”‚                                        â”‚  â”‚
â”‚  â”‚                             â”‚ ToolRegistry.get(tool_name)            â”‚  â”‚
â”‚  â”‚                             â–¼                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Layer 3: Tool Layer                                      â”‚  â”‚
â”‚  â”‚                   (å·¥å…·å±‚)                                             â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚  â”‚ ToolInterface  â”‚  â”‚ ToolInterface  â”‚  â”‚ ToolInterface  â”‚          â”‚  â”‚
â”‚  â”‚  â”‚ (ç»Ÿä¸€æ¥å£)      â”‚  â”‚ (ç»Ÿä¸€æ¥å£)      â”‚  â”‚ (ç»Ÿä¸€æ¥å£)      â”‚          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â”‚          â”‚                   â”‚                   â”‚                   â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚   â”‚ Local Impl  â”‚     â”‚ Cloud Impl  â”‚     â”‚ Mock Impl   â”‚            â”‚  â”‚
â”‚  â”‚   â”‚ (æœ¬åœ°å®ç°)   â”‚     â”‚ (äº‘ç«¯å®ç°)   â”‚     â”‚ (æµ‹è¯•å®ç°)   â”‚            â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ç«¯äº‘ä»£ç å…±äº«ç­–ç•¥

### 2.1 æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| **Monorepo + uv Workspace** âœ… | ç‰ˆæœ¬åŒæ­¥è‡ªåŠ¨ã€å¼€å‘ä½“éªŒå¥½ã€éƒ¨ç½²çµæ´» | ä»“åº“è¾ƒå¤§ | **æ¨èæ–¹æ¡ˆ** |
| ç‹¬ç«‹ä»“åº“ + PyPI | è§£è€¦æ¸…æ™° | ç‰ˆæœ¬åŒæ­¥å›°éš¾ã€å‘å¸ƒæµç¨‹å¤æ‚ | å¼€æºå…±äº« |
| Git Submodule | ä»£ç éš”ç¦» | ç®¡ç†å¤æ‚ã€å¼€å‘ä½“éªŒå·® | å¤šå›¢é˜Ÿåä½œ |
| å¤åˆ¶ä»£ç  | ç®€å• | ç»´æŠ¤å™©æ¢¦ | âŒ ä¸æ¨è |

### 2.2 æ¨èæ¶æ„ï¼šMonorepo + uv Workspace

```text
ai-creator/                              # Monorepo æ ¹ç›®å½•
â”œâ”€â”€ pyproject.toml                       # Workspace å®šä¹‰
â”œâ”€â”€ uv.lock                              # é”å®šä¾èµ–ç‰ˆæœ¬
â”‚
â”œâ”€â”€ packages/                            # ğŸ”¥ å…±äº«åŒ…ç›®å½•
â”‚   â””â”€â”€ agent-core/                      # æ ¸å¿ƒå…±äº«åŒ…
â”‚       â”œâ”€â”€ pyproject.toml               # åŒ…å®šä¹‰
â”‚       â””â”€â”€ src/
â”‚           â””â”€â”€ agent_core/              # Python åŒ…
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ graph/               # Graph åŠ è½½/ç¼–è¯‘
â”‚               â”œâ”€â”€ runtime/             # è¿è¡Œæ—¶æ¥å£
â”‚               â”œâ”€â”€ tools/               # å·¥å…·å±‚åŸºç±»
â”‚               â”œâ”€â”€ resource/            # èµ„æºç®¡ç†
â”‚               â””â”€â”€ crypto/              # åŠ å¯†å·¥å…·
â”‚
â”œâ”€â”€ apps/                                # ğŸ”¥ åº”ç”¨ç›®å½•
â”‚   â”œâ”€â”€ sidecar/                         # æ¡Œé¢ç«¯ Python Sidecar
â”‚   â”‚   â”œâ”€â”€ pyproject.toml               # ä¾èµ– agent-core
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â””â”€â”€ sidecar/
â”‚   â”‚           â”œâ”€â”€ executor.py          # LocalExecutor
â”‚   â”‚           â”œâ”€â”€ tools/               # æœ¬åœ°å·¥å…·å®ç°
â”‚   â”‚           â””â”€â”€ main.py
â”‚   â”‚
â”‚   â””â”€â”€ cloud-backend/                   # äº‘ç«¯æœåŠ¡
â”‚       â”œâ”€â”€ pyproject.toml               # ä¾èµ– agent-core
â”‚       â””â”€â”€ backend/
â”‚           â””â”€â”€ app/
â”‚               â””â”€â”€ agent/
â”‚                   â”œâ”€â”€ executor.py      # CloudExecutor
â”‚                   â””â”€â”€ tools/           # äº‘ç«¯å·¥å…·å®ç°
â”‚
â”œâ”€â”€ agent-definitions/                   # Graph å®šä¹‰ï¼ˆå…±äº«ï¼‰
â”‚   â”œâ”€â”€ content-creation.yaml
â”‚   â””â”€â”€ publish-workflow.yaml
â”‚
â””â”€â”€ apps/desktop/                        # Tauri æ¡Œé¢åº”ç”¨
    â””â”€â”€ src-tauri/
        â””â”€â”€ sidecar/                     # Sidecar äºŒè¿›åˆ¶
```

### 2.3 Workspace é…ç½®

```toml
# ai-creator/pyproject.toml (Monorepo æ ¹)

[project]
name = "ai-creator-workspace"
version = "0.1.0"
requires-python = ">=3.11"

[tool.uv]
# å®šä¹‰ workspace æˆå‘˜
workspace = { members = ["packages/*", "apps/*"] }

[tool.uv.sources]
# æœ¬åœ°åŒ…æºå®šä¹‰ï¼ˆæ‰€æœ‰ workspace æˆå‘˜è‡ªåŠ¨å¯ç”¨ï¼‰
agent-core = { workspace = true }
```

```toml
# packages/agent-core/pyproject.toml

[project]
name = "agent-core"
version = "0.1.0"
description = "AI Creator Agent Core - ç«¯äº‘å…±äº«æ ¸å¿ƒ"
requires-python = ">=3.11"

dependencies = [
    "langgraph>=0.2.0",
    "pydantic>=2.0.0",
    "pyyaml>=6.0.0",
    "anthropic>=0.40.0",
    "cryptography>=42.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/agent_core"]
```

```toml
# apps/sidecar/pyproject.toml

[project]
name = "ai-creator-sidecar"
version = "0.1.0"
description = "AI Creator æ¡Œé¢ç«¯ Python Sidecar"
requires-python = ">=3.11"

dependencies = [
    "agent-core",           # ğŸ”¥ workspace å¼•ç”¨ï¼Œæ— éœ€ç‰ˆæœ¬å·
    "playwright>=1.40.0",
    "uvicorn>=0.30.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

```toml
# services/cloud-backend/pyproject.toml

[project]
name = "ai-creator-cloud"
version = "0.1.0"
description = "AI Creator äº‘ç«¯æœåŠ¡"
requires-python = ">=3.11"

dependencies = [
    "agent-core",           # ğŸ”¥ workspace å¼•ç”¨
    "fastapi>=0.115.0",
    "celery>=5.4.0",
    "redis>=5.0.0",
    "sqlalchemy>=2.0.0",
]
```

### 2.4 å¼€å‘å·¥ä½œæµ

```bash
# 1. åˆå§‹åŒ– workspace
cd ai-creator
uv sync                                 # å®‰è£…æ‰€æœ‰ä¾èµ–

# 2. å¼€å‘ agent-coreï¼ˆä¿®æ”¹å…±äº«ä»£ç ï¼‰
cd packages/agent-core
# ä¿®æ”¹ä»£ç ...
uv run pytest                           # æµ‹è¯•

# 3. å¼€å‘ sidecarï¼ˆè‡ªåŠ¨ä½¿ç”¨æœ€æ–° agent-coreï¼‰
cd apps/sidecar
uv run python -m sidecar.main           # è¿è¡Œ

# 4. å¼€å‘äº‘ç«¯ï¼ˆè‡ªåŠ¨ä½¿ç”¨æœ€æ–° agent-coreï¼‰
cd services/cloud-backend
uv run uvicorn backend.app.main:app     # è¿è¡Œ

# 5. æ„å»º sidecar å‘å¸ƒåŒ…
cd apps/sidecar
uv build                                # ç”Ÿæˆ wheel/tar.gz
# æˆ–ä½¿ç”¨ PyInstaller æ‰“åŒ…äºŒè¿›åˆ¶
pyinstaller --onefile src/sidecar/main.py
```

---

## 3. æ ¸å¿ƒä»£ç è®¾è®¡

### 3.1 agent-core åŒ…ç»“æ„

```text
packages/agent-core/src/agent_core/
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ graph/                               # Layer 1: Graph å®šä¹‰å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                        # GraphLoader: åŠ è½½ YAML/JSON
â”‚   â”œâ”€â”€ compiler.py                      # GraphCompiler: ç¼–è¯‘ä¸º LangGraph
â”‚   â”œâ”€â”€ validator.py                     # éªŒè¯ Graph å®šä¹‰
â”‚   â””â”€â”€ types.py                         # Graph ç±»å‹å®šä¹‰
â”‚
â”œâ”€â”€ runtime/                             # Layer 2: è¿è¡Œæ—¶å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interfaces.py                    # ExecutorInterface æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ context.py                       # RuntimeContext è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
â”‚   â”œâ”€â”€ events.py                        # AgentEvent äº‹ä»¶å®šä¹‰
â”‚   â””â”€â”€ router.py                        # RuntimeRouter æ™ºèƒ½è·¯ç”±
â”‚
â”œâ”€â”€ tools/                               # Layer 3: å·¥å…·å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                          # ToolInterface æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ registry.py                      # ToolRegistry å·¥å…·æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ capability.py                    # èƒ½åŠ›å£°æ˜
â”‚   â”‚
â”‚   â”œâ”€â”€ builtin/                         # å†…ç½®å·¥å…·ï¼ˆç«¯äº‘å…±ç”¨ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm.py                       # LLM å·¥å…·
â”‚   â”‚   â”œâ”€â”€ web_search.py                # ç½‘ç»œæœç´¢
â”‚   â”‚   â””â”€â”€ storage.py                   # å­˜å‚¨å·¥å…·
â”‚   â”‚
â”‚   â””â”€â”€ stubs/                           # å·¥å…·æ¡©ï¼ˆéœ€è¦ç«¯/äº‘å®ç°ï¼‰
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ browser.py                   # æµè§ˆå™¨å·¥å…·æ¥å£
â”‚       â””â”€â”€ credential.py                # å‡­è¯å·¥å…·æ¥å£
â”‚
â”œâ”€â”€ resource/                            # èµ„æºç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ uri.py                           # AssetURI ç»Ÿä¸€èµ„æºæ ‡è¯†
â”‚   â””â”€â”€ resolver.py                      # AssetResolver æŠ½è±¡åŸºç±»
â”‚
â”œâ”€â”€ crypto/                              # åŠ å¯†å·¥å…·
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ credential_crypto.py             # å‡­è¯åŠ å¯†
â”‚
â””â”€â”€ platforms/                           # å¹³å°é€‚é…å™¨
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py                          # PlatformAdapter åŸºç±»
    â”œâ”€â”€ xiaohongshu.py
    â”œâ”€â”€ douyin.py
    â””â”€â”€ ...
```

### 3.2 è¿è¡Œæ—¶æ¥å£è®¾è®¡

```python
# packages/agent-core/src/agent_core/runtime/interfaces.py

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Optional, AsyncIterator
from dataclasses import dataclass, field
from datetime import datetime

class RuntimeType(Enum):
    """è¿è¡Œæ—¶ç±»å‹"""
    LOCAL = "local"       # æœ¬åœ° Python Sidecar
    CLOUD = "cloud"       # äº‘ç«¯æœåŠ¡


@dataclass
class RuntimeContext:
    """è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ - ç»Ÿä¸€ç«¯äº‘é…ç½®æ³¨å…¥"""
    runtime_type: RuntimeType
    user_id: str
    inputs: dict

    # æ¨¡å‹é…ç½®
    model_default: str = "claude-sonnet-4-20250514"
    model_fast: str = "claude-3-5-haiku-20241022"

    # API å¯†é’¥ï¼ˆè¿è¡Œæ—¶æ³¨å…¥ï¼Œä¸ç¡¬ç¼–ç ï¼‰
    api_keys: dict = field(default_factory=dict)

    # èµ„æºè§£æå™¨ï¼ˆç”±ç«¯/äº‘å®ç°æ³¨å…¥ï¼‰
    asset_resolver: Optional["AssetResolver"] = None

    # é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆç«¯/äº‘ç‰¹æœ‰ï¼‰
    extra: dict = field(default_factory=dict)


@dataclass
class ExecutionRequest:
    """æ‰§è¡Œè¯·æ±‚"""
    graph_name: str                    # Graph åç§°
    inputs: dict                       # è¾“å…¥å‚æ•°
    user_id: str                       # ç”¨æˆ·ID
    session_id: Optional[str] = None   # ä¼šè¯IDï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    timeout: int = 300                 # è¶…æ—¶æ—¶é—´(ç§’)
    trace_id: Optional[str] = None     # è¿½è¸ªID


@dataclass
class ExecutionResponse:
    """æ‰§è¡Œå“åº”"""
    success: bool
    outputs: Any
    error: Optional[str] = None
    execution_id: str = ""
    execution_time: float = 0.0
    runtime_type: RuntimeType = RuntimeType.LOCAL
    trace_id: str = ""


@dataclass
class AgentEvent:
    """Agent æ‰§è¡Œäº‹ä»¶"""
    event_type: str          # node_started | tool_called | completed | failed
    run_id: str
    trace_id: str
    timestamp: datetime
    data: dict

    # Token ä½¿ç”¨è¿½è¸ª
    tokens_used: int = 0
    cost_cents: int = 0


class ExecutorInterface(ABC):
    """æ‰§è¡Œå™¨æ¥å£ - ç«¯äº‘ç»Ÿä¸€"""

    runtime_type: RuntimeType

    @abstractmethod
    async def execute(self, request: ExecutionRequest) -> ExecutionResponse:
        """åŒæ­¥æ‰§è¡Œ Graph"""
        pass

    @abstractmethod
    async def execute_stream(
        self, request: ExecutionRequest
    ) -> AsyncIterator[AgentEvent]:
        """æµå¼æ‰§è¡Œ Graphï¼Œè¿”å›äº‹ä»¶æµ"""
        pass

    @abstractmethod
    async def get_status(self, execution_id: str) -> dict:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        pass

    @abstractmethod
    async def cancel(self, execution_id: str) -> bool:
        """å–æ¶ˆæ‰§è¡Œ"""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass
```

### 3.3 å·¥å…·å±‚è®¾è®¡

```python
# packages/agent-core/src/agent_core/tools/base.py

from abc import ABC, abstractmethod
from typing import Any, Optional, ClassVar
from dataclasses import dataclass, field
from enum import Enum

from ..runtime.interfaces import RuntimeType


class ToolCapability(Enum):
    """å·¥å…·èƒ½åŠ›æ ‡è¯†"""
    LLM_GENERATE = "llm_generate"
    WEB_SEARCH = "web_search"
    IMAGE_GEN = "image_gen"
    BROWSER_AUTOMATION = "browser_automation"
    FILE_STORAGE = "file_storage"
    CREDENTIAL_STORE = "credential_store"
    HOT_TOPIC_DISCOVERY = "hot_topic_discovery"      # BettaFish çƒ­ç‚¹å‘ç°
    SENTIMENT_ANALYSIS = "sentiment_analysis"         # BettaFish æƒ…æ„Ÿåˆ†æ


@dataclass
class ToolMetadata:
    """å·¥å…·å…ƒæ•°æ®"""
    name: str
    description: str
    capabilities: list[ToolCapability]
    supported_runtimes: list[RuntimeType] = field(
        default_factory=lambda: [RuntimeType.LOCAL, RuntimeType.CLOUD]
    )
    fallback_tool: Optional[str] = None
    requires_auth: bool = False


@dataclass
class ToolResult:
    """å·¥å…·æ‰§è¡Œç»“æœ"""
    success: bool
    data: Any
    error: Optional[str] = None


class ToolInterface(ABC):
    """å·¥å…·åŸºç±» - ç»Ÿä¸€æ¥å£"""

    metadata: ClassVar[ToolMetadata]

    @abstractmethod
    async def execute(self, ctx: "RuntimeContext", **kwargs) -> ToolResult:
        """æ‰§è¡Œå·¥å…· - æ¥æ”¶è¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
        pass

    @abstractmethod
    def get_schema(self) -> dict:
        """è·å–è¾“å…¥å‚æ•° JSON Schema"""
        pass

    def is_available(self, ctx: "RuntimeContext") -> bool:
        """æ£€æŸ¥å·¥å…·åœ¨å½“å‰è¿è¡Œæ—¶æ˜¯å¦å¯ç”¨"""
        return ctx.runtime_type in self.metadata.supported_runtimes
```

```python
# packages/agent-core/src/agent_core/tools/registry.py

from typing import Type, Optional, Dict
from .base import ToolInterface, ToolCapability
from ..runtime.interfaces import RuntimeType, RuntimeContext


class ToolRegistry:
    """
    å·¥å…·æ³¨å†Œè¡¨ - ç«¯äº‘ç»Ÿä¸€ç®¡ç†

    ä½¿ç”¨æ–¹å¼:
        # æ³¨å†Œå·¥å…·ï¼ˆåœ¨ç«¯/äº‘å¯åŠ¨æ—¶ï¼‰
        ToolRegistry.register("browser_publish", RuntimeType.LOCAL)(LocalBrowserTool)
        ToolRegistry.register("browser_publish", RuntimeType.CLOUD)(CloudBrowserTool)

        # è·å–å·¥å…·ï¼ˆåœ¨æ‰§è¡Œæ—¶ï¼‰
        registry = ToolRegistry(runtime_type=RuntimeType.LOCAL)
        tool = registry.get("browser_publish")
    """

    # å…¨å±€æ³¨å†Œè¡¨: {tool_name: {runtime_type: tool_class}}
    _tools: Dict[str, Dict[RuntimeType, Type[ToolInterface]]] = {}

    @classmethod
    def register(cls, name: str, runtime_type: RuntimeType):
        """è£…é¥°å™¨ï¼šæ³¨å†Œå·¥å…·åˆ°æŒ‡å®šè¿è¡Œæ—¶"""
        def decorator(tool_class: Type[ToolInterface]):
            if name not in cls._tools:
                cls._tools[name] = {}
            cls._tools[name][runtime_type] = tool_class
            return tool_class
        return decorator

    @classmethod
    def register_universal(cls, name: str):
        """è£…é¥°å™¨ï¼šæ³¨å†Œå·¥å…·åˆ°æ‰€æœ‰è¿è¡Œæ—¶ï¼ˆç«¯äº‘å…±ç”¨ï¼‰"""
        def decorator(tool_class: Type[ToolInterface]):
            if name not in cls._tools:
                cls._tools[name] = {}
            cls._tools[name][RuntimeType.LOCAL] = tool_class
            cls._tools[name][RuntimeType.CLOUD] = tool_class
            return tool_class
        return decorator

    def __init__(self, runtime_type: RuntimeType):
        self.runtime_type = runtime_type
        self._instances: Dict[str, ToolInterface] = {}

    def get(
        self,
        name: str,
        ctx: Optional[RuntimeContext] = None
    ) -> Optional[ToolInterface]:
        """è·å–å·¥å…·å®ä¾‹"""
        rt = ctx.runtime_type if ctx else self.runtime_type

        cache_key = f"{name}:{rt.value}"
        if cache_key in self._instances:
            return self._instances[cache_key]

        if name not in self._tools:
            return None

        tool_class = self._tools[name].get(rt)
        if tool_class is None:
            return None

        instance = tool_class()
        self._instances[cache_key] = instance
        return instance

    def list_tools(self) -> list[str]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·"""
        return list(self._tools.keys())

    def list_available_tools(self, ctx: RuntimeContext) -> list[str]:
        """åˆ—å‡ºå½“å‰è¿è¡Œæ—¶å¯ç”¨çš„å·¥å…·"""
        available = []
        for name in self._tools:
            tool = self.get(name, ctx)
            if tool and tool.is_available(ctx):
                available.append(name)
        return available
```

### 3.4 å†…ç½®å·¥å…·ç¤ºä¾‹ï¼ˆç«¯äº‘å…±ç”¨ï¼‰

```python
# packages/agent-core/src/agent_core/tools/builtin/llm.py

from ..base import ToolInterface, ToolMetadata, ToolCapability, ToolResult
from ..registry import ToolRegistry
from ...runtime.interfaces import RuntimeContext


@ToolRegistry.register_universal("llm_generate")
class LLMGenerateTool(ToolInterface):
    """LLM æ–‡æœ¬ç”Ÿæˆå·¥å…· - ç«¯äº‘å…±ç”¨å®ç°"""

    metadata = ToolMetadata(
        name="llm_generate",
        description="ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬",
        capabilities=[ToolCapability.LLM_GENERATE],
    )

    async def execute(
        self,
        ctx: RuntimeContext,
        *,
        prompt: str,
        system: str = "",
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
    ) -> ToolResult:
        """æ‰§è¡Œ LLM ç”Ÿæˆ"""
        from anthropic import Anthropic

        # ä»è¿è¡Œæ—¶ä¸Šä¸‹æ–‡è·å– API Key
        api_key = ctx.api_keys.get("anthropic")
        if not api_key:
            return ToolResult(success=False, data=None, error="Missing Anthropic API key")

        # ä½¿ç”¨é…ç½®çš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        model = model or ctx.model_default

        try:
            client = Anthropic(api_key=api_key)
            response = client.messages.create(
                model=model,
                max_tokens=max_tokens,
                system=system,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
            )

            return ToolResult(
                success=True,
                data={
                    "content": response.content[0].text,
                    "usage": {
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens,
                    }
                }
            )
        except Exception as e:
            return ToolResult(success=False, data=None, error=str(e))

    def get_schema(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "prompt": {"type": "string", "description": "ç”¨æˆ·æç¤º"},
                "system": {"type": "string", "description": "ç³»ç»Ÿæç¤º"},
                "model": {"type": "string", "description": "æ¨¡å‹åç§°"},
                "max_tokens": {"type": "integer", "default": 4096},
                "temperature": {"type": "number", "default": 0.7},
            },
            "required": ["prompt"]
        }
```

### 3.5 ç«¯ä¾§ä¸“ç”¨å·¥å…·ï¼ˆSidecar å®ç°ï¼‰

```python
# apps/sidecar/src/sidecar/tools/browser.py

from agent_core.tools.base import ToolInterface, ToolMetadata, ToolCapability, ToolResult
from agent_core.tools.registry import ToolRegistry
from agent_core.runtime.interfaces import RuntimeType, RuntimeContext


@ToolRegistry.register("browser_publish", RuntimeType.LOCAL)
class LocalBrowserPublishTool(ToolInterface):
    """æœ¬åœ°æµè§ˆå™¨å‘å¸ƒå·¥å…· - ä»…ç«¯ä¾§å¯ç”¨"""

    metadata = ToolMetadata(
        name="browser_publish",
        description="ä½¿ç”¨æœ¬åœ° Playwright æµè§ˆå™¨å‘å¸ƒå†…å®¹",
        capabilities=[ToolCapability.BROWSER_AUTOMATION],
        supported_runtimes=[RuntimeType.LOCAL],
    )

    async def execute(
        self,
        ctx: RuntimeContext,
        *,
        platform: str,
        account_id: str,
        content: dict,
    ) -> ToolResult:
        """ä½¿ç”¨æœ¬åœ° Playwright å‘å¸ƒ"""
        from agent_core.platforms import get_adapter
        from playwright.async_api import async_playwright

        try:
            # ä»ä¸Šä¸‹æ–‡è·å–æµè§ˆå™¨ç®¡ç†å™¨
            browser_manager = ctx.extra.get("browser_manager")

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=False)
                context = await browser.new_context()

                # åŠ è½½æœ¬åœ°ä¿å­˜çš„å‡­è¯
                await self._load_credentials(context, platform, account_id)

                page = await context.new_page()
                adapter = get_adapter(platform)
                result = await adapter.publish(page, content)

                await browser.close()
                return ToolResult(success=True, data=result)

        except Exception as e:
            return ToolResult(success=False, data=None, error=str(e))

    async def _load_credentials(self, context, platform: str, account_id: str):
        """åŠ è½½æœ¬åœ°åŠ å¯†çš„å‡­è¯"""
        # ä»æœ¬åœ°å­˜å‚¨è§£å¯†å¹¶åŠ è½½å‡­è¯
        pass

    def get_schema(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "platform": {"type": "string", "enum": ["xiaohongshu", "douyin", "weibo"]},
                "account_id": {"type": "string"},
                "content": {"type": "object"},
            },
            "required": ["platform", "account_id", "content"]
        }
```

### 3.6 äº‘ç«¯ä¸“ç”¨å·¥å…·ï¼ˆBackend å®ç°ï¼‰

```python
# services/cloud-backend/backend/app/agent/tools/browser.py

from agent_core.tools.base import ToolInterface, ToolMetadata, ToolCapability, ToolResult
from agent_core.tools.registry import ToolRegistry
from agent_core.runtime.interfaces import RuntimeType, RuntimeContext


@ToolRegistry.register("browser_publish", RuntimeType.CLOUD)
class CloudBrowserPublishTool(ToolInterface):
    """äº‘ç«¯æµè§ˆå™¨å‘å¸ƒå·¥å…· - ä½¿ç”¨æµè§ˆå™¨æ± """

    metadata = ToolMetadata(
        name="browser_publish",
        description="ä½¿ç”¨äº‘ç«¯æµè§ˆå™¨æ± å‘å¸ƒå†…å®¹",
        capabilities=[ToolCapability.BROWSER_AUTOMATION],
        supported_runtimes=[RuntimeType.CLOUD],
    )

    async def execute(
        self,
        ctx: RuntimeContext,
        *,
        platform: str,
        account_id: str,
        content: dict,
    ) -> ToolResult:
        """ä½¿ç”¨äº‘ç«¯æµè§ˆå™¨æ± å‘å¸ƒ"""
        from backend.app.browser.pool import BrowserPool

        try:
            # ä»ä¸Šä¸‹æ–‡è·å–æµè§ˆå™¨æ± 
            browser_pool: BrowserPool = ctx.extra.get("browser_pool")

            # è·å–ç”¨æˆ·åŒæ­¥çš„å‡­è¯ï¼ˆéœ€è¦ç”¨æˆ·å¼€å¯å‡­è¯åŒæ­¥ï¼‰
            credential = await self._get_synced_credential(
                ctx.user_id, platform, account_id
            )
            if not credential:
                return ToolResult(
                    success=False,
                    data=None,
                    error="æœªæ‰¾åˆ°åŒæ­¥çš„å‡­è¯ï¼Œè¯·åœ¨æ¡Œé¢ç«¯å¼€å¯å‡­è¯åŒæ­¥"
                )

            # ä»æµè§ˆå™¨æ± è·å–å®ä¾‹
            browser_context = await browser_pool.acquire(platform, credential)

            try:
                result = await browser_context.publish(content)
                return ToolResult(success=True, data=result)
            finally:
                await browser_pool.release(browser_context)

        except Exception as e:
            return ToolResult(success=False, data=None, error=str(e))

    async def _get_synced_credential(self, user_id: str, platform: str, account_id: str):
        """è·å–ç”¨æˆ·åŒæ­¥çš„å‡­è¯"""
        # ä»æ•°æ®åº“æŸ¥è¯¢åŠ å¯†çš„å‡­è¯
        pass

    def get_schema(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "platform": {"type": "string"},
                "account_id": {"type": "string"},
                "content": {"type": "object"},
            },
            "required": ["platform", "account_id", "content"]
        }
```

---

## 4. æœ¬åœ°æ‰§è¡Œå™¨å®ç°

```python
# apps/sidecar/src/sidecar/executor.py

import time
import uuid
from typing import AsyncIterator

from agent_core.runtime.interfaces import (
    ExecutorInterface, RuntimeType, RuntimeContext,
    ExecutionRequest, ExecutionResponse, AgentEvent
)
from agent_core.graph.loader import GraphLoader
from agent_core.graph.compiler import GraphCompiler
from agent_core.tools.registry import ToolRegistry
from agent_core.resource.resolver import LocalAssetResolver


class LocalExecutor(ExecutorInterface):
    """æœ¬åœ°æ‰§è¡Œå™¨ - æ¡Œé¢ç«¯ Python Sidecar"""

    runtime_type = RuntimeType.LOCAL

    def __init__(self, config: dict):
        self.config = config
        self.graph_loader = GraphLoader(
            definitions_path=config.get('definitions_path', 'agent-definitions')
        )
        self.tool_registry = ToolRegistry(RuntimeType.LOCAL)
        self.compiler = GraphCompiler(self.tool_registry)
        self._executions: dict[str, dict] = {}

    def _create_context(self, request: ExecutionRequest) -> RuntimeContext:
        """åˆ›å»ºè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
        return RuntimeContext(
            runtime_type=RuntimeType.LOCAL,
            user_id=request.user_id,
            inputs=request.inputs,
            model_default=self.config.get('default_model', 'claude-sonnet-4-20250514'),
            model_fast=self.config.get('fast_model', 'claude-3-5-haiku-20241022'),
            api_keys={
                "anthropic": self.config.get('anthropic_api_key'),
            },
            asset_resolver=LocalAssetResolver(self.config),
            extra={
                "browser_manager": self._get_browser_manager(),
            }
        )

    async def execute(self, request: ExecutionRequest) -> ExecutionResponse:
        """æ‰§è¡Œ Graph"""
        execution_id = str(uuid.uuid4())
        trace_id = request.trace_id or f"tr-{uuid.uuid4()}"
        start_time = time.time()

        try:
            # åŠ è½½ Graph å®šä¹‰
            definition = self.graph_loader.load(request.graph_name)

            # åˆ›å»ºè¿è¡Œæ—¶ä¸Šä¸‹æ–‡
            ctx = self._create_context(request)

            # ç¼–è¯‘å¹¶æ‰§è¡Œ
            graph = self.compiler.compile(definition, ctx)
            initial_state = self._create_initial_state(definition['spec']['state'])
            final_state = await graph.ainvoke(initial_state)

            # æå–è¾“å‡º
            outputs = self._extract_outputs(definition['spec']['outputs'], final_state)

            return ExecutionResponse(
                success=True,
                outputs=outputs,
                execution_id=execution_id,
                execution_time=time.time() - start_time,
                runtime_type=self.runtime_type,
                trace_id=trace_id,
            )

        except Exception as e:
            return ExecutionResponse(
                success=False,
                outputs=None,
                error=str(e),
                execution_id=execution_id,
                execution_time=time.time() - start_time,
                runtime_type=self.runtime_type,
                trace_id=trace_id,
            )

    async def execute_stream(
        self, request: ExecutionRequest
    ) -> AsyncIterator[AgentEvent]:
        """æµå¼æ‰§è¡Œï¼Œè¿”å›äº‹ä»¶æµ"""
        execution_id = str(uuid.uuid4())
        trace_id = request.trace_id or f"tr-{uuid.uuid4()}"

        # å‘é€å¼€å§‹äº‹ä»¶
        yield AgentEvent(
            event_type="run_started",
            run_id=execution_id,
            trace_id=trace_id,
            timestamp=datetime.now(),
            data={"graph_name": request.graph_name}
        )

        try:
            definition = self.graph_loader.load(request.graph_name)
            ctx = self._create_context(request)
            graph = self.compiler.compile(definition, ctx)

            # ä½¿ç”¨ astream_events è·å–è¯¦ç»†äº‹ä»¶
            initial_state = self._create_initial_state(definition['spec']['state'])

            async for event in graph.astream_events(initial_state):
                yield AgentEvent(
                    event_type=event["event"],
                    run_id=execution_id,
                    trace_id=trace_id,
                    timestamp=datetime.now(),
                    data=event.get("data", {}),
                )

            yield AgentEvent(
                event_type="run_completed",
                run_id=execution_id,
                trace_id=trace_id,
                timestamp=datetime.now(),
                data={}
            )

        except Exception as e:
            yield AgentEvent(
                event_type="run_failed",
                run_id=execution_id,
                trace_id=trace_id,
                timestamp=datetime.now(),
                data={"error": str(e)}
            )

    async def get_status(self, execution_id: str) -> dict:
        return self._executions.get(execution_id, {"status": "not_found"})

    async def cancel(self, execution_id: str) -> bool:
        if execution_id in self._executions:
            self._executions[execution_id]["status"] = "cancelled"
            return True
        return False

    async def health_check(self) -> bool:
        return True

    def _get_browser_manager(self):
        """è·å–æµè§ˆå™¨ç®¡ç†å™¨"""
        # è¿”å›æœ¬åœ°æµè§ˆå™¨ç®¡ç†å™¨å®ä¾‹
        pass

    def _create_initial_state(self, state_spec: dict) -> dict:
        """åˆ›å»ºåˆå§‹çŠ¶æ€"""
        state = {}
        for key, spec in state_spec.items():
            if 'initial' in spec:
                state[key] = spec['initial']
            elif spec['type'] == 'array':
                state[key] = []
            elif spec['type'] == 'string':
                state[key] = ""
            else:
                state[key] = None
        return state

    def _extract_outputs(self, outputs_spec: dict, final_state: dict) -> dict:
        """ä»æœ€ç»ˆçŠ¶æ€æå–è¾“å‡º"""
        # è§£æ ${state.xxx} å¼•ç”¨
        pass
```

---

## 5. äº‘ç«¯æ‰§è¡Œå™¨å®ç°

```python
# services/cloud-backend/backend/app/agent/executor.py

import time
import uuid
import json
from typing import AsyncIterator

from agent_core.runtime.interfaces import (
    ExecutorInterface, RuntimeType, RuntimeContext,
    ExecutionRequest, ExecutionResponse, AgentEvent
)
from agent_core.graph.loader import GraphLoader
from agent_core.graph.compiler import GraphCompiler
from agent_core.tools.registry import ToolRegistry
from agent_core.resource.resolver import CloudAssetResolver

from celery import shared_task


class CloudExecutor(ExecutorInterface):
    """äº‘ç«¯æ‰§è¡Œå™¨ - FastAPI åç«¯"""

    runtime_type = RuntimeType.CLOUD

    def __init__(self, db, redis, config: dict, browser_pool=None):
        self.db = db
        self.redis = redis
        self.config = config
        self.browser_pool = browser_pool
        self.graph_loader = GraphLoader(
            definitions_path=config.get('definitions_path', 'agent-definitions')
        )
        self.tool_registry = ToolRegistry(RuntimeType.CLOUD)
        self.compiler = GraphCompiler(self.tool_registry)

    def _create_context(self, request: ExecutionRequest) -> RuntimeContext:
        """åˆ›å»ºè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
        return RuntimeContext(
            runtime_type=RuntimeType.CLOUD,
            user_id=request.user_id,
            inputs=request.inputs,
            model_default=self._get_user_model(request.user_id),
            model_fast='claude-3-5-haiku-20241022',
            api_keys={
                "anthropic": self.config.get('anthropic_api_key'),
            },
            asset_resolver=CloudAssetResolver(self.config, request.user_id),
            extra={
                "db": self.db,
                "redis": self.redis,
                "browser_pool": self.browser_pool,
            }
        )

    async def execute(self, request: ExecutionRequest) -> ExecutionResponse:
        """åŒæ­¥æ‰§è¡Œï¼ˆçŸ­ä»»åŠ¡ï¼‰"""
        execution_id = str(uuid.uuid4())
        trace_id = request.trace_id or f"tr-{uuid.uuid4()}"
        start_time = time.time()

        try:
            definition = self.graph_loader.load(request.graph_name)
            ctx = self._create_context(request)
            graph = self.compiler.compile(definition, ctx)
            initial_state = self._create_initial_state(definition['spec']['state'])
            final_state = await graph.ainvoke(initial_state)
            outputs = self._extract_outputs(definition['spec']['outputs'], final_state)

            return ExecutionResponse(
                success=True,
                outputs=outputs,
                execution_id=execution_id,
                execution_time=time.time() - start_time,
                runtime_type=self.runtime_type,
                trace_id=trace_id,
            )

        except Exception as e:
            return ExecutionResponse(
                success=False,
                outputs=None,
                error=str(e),
                execution_id=execution_id,
                execution_time=time.time() - start_time,
                runtime_type=self.runtime_type,
                trace_id=trace_id,
            )

    async def execute_async(self, request: ExecutionRequest) -> str:
        """å¼‚æ­¥æ‰§è¡Œï¼ˆé•¿ä»»åŠ¡ï¼Œæäº¤åˆ° Celeryï¼‰"""
        execution_id = str(uuid.uuid4())
        trace_id = request.trace_id or f"tr-{uuid.uuid4()}"

        # ä¿å­˜åˆå§‹çŠ¶æ€
        await self.redis.hset(f"execution:{execution_id}", mapping={
            "status": "pending",
            "trace_id": trace_id,
            "graph_name": request.graph_name,
            "created_at": time.time(),
        })

        # æäº¤åˆ° Celery é˜Ÿåˆ—
        execute_graph_task.delay(
            execution_id=execution_id,
            trace_id=trace_id,
            graph_name=request.graph_name,
            inputs=request.inputs,
            user_id=request.user_id,
        )

        return execution_id

    async def execute_stream(
        self, request: ExecutionRequest
    ) -> AsyncIterator[AgentEvent]:
        """æµå¼æ‰§è¡Œï¼Œé€šè¿‡ SSE è¿”å›"""
        # å®ç°ç±»ä¼¼ LocalExecutor çš„æµå¼æ‰§è¡Œ
        pass

    def _get_user_model(self, user_id: str) -> str:
        """æ ¹æ®ç”¨æˆ·è®¢é˜…çº§åˆ«è·å–æ¨¡å‹"""
        # æŸ¥è¯¢ç”¨æˆ·è®¢é˜…çº§åˆ«
        # Pro ç”¨æˆ·å¯ç”¨ claude-opus-4-20250514
        return 'claude-sonnet-4-20250514'

    async def get_status(self, execution_id: str) -> dict:
        status = await self.redis.hgetall(f"execution:{execution_id}")
        return status or {"status": "not_found"}

    async def cancel(self, execution_id: str) -> bool:
        await self.redis.set(f"execution:{execution_id}:cancel", "1", ex=3600)
        return True

    async def health_check(self) -> bool:
        try:
            await self.redis.ping()
            return True
        except Exception:
            return False


@shared_task(bind=True)
def execute_graph_task(
    self,
    execution_id: str,
    trace_id: str,
    graph_name: str,
    inputs: dict,
    user_id: str
):
    """Celery å¼‚æ­¥ä»»åŠ¡"""
    import asyncio

    async def run():
        from backend.app.core.deps import get_db, get_redis, get_browser_pool
        from backend.app.core.config import settings

        async with get_db() as db:
            redis = await get_redis()
            browser_pool = await get_browser_pool()

            executor = CloudExecutor(db, redis, settings.dict(), browser_pool)

            # æ›´æ–°çŠ¶æ€
            await redis.hset(f"execution:{execution_id}", mapping={
                "status": "running",
                "started_at": time.time(),
            })

            try:
                request = ExecutionRequest(
                    graph_name=graph_name,
                    inputs=inputs,
                    user_id=user_id,
                    trace_id=trace_id,
                )
                response = await executor.execute(request)

                await redis.hset(f"execution:{execution_id}", mapping={
                    "status": "completed" if response.success else "failed",
                    "outputs": json.dumps(response.outputs),
                    "error": response.error or "",
                    "completed_at": time.time(),
                })

            except Exception as e:
                await redis.hset(f"execution:{execution_id}", mapping={
                    "status": "failed",
                    "error": str(e),
                    "completed_at": time.time(),
                })

    asyncio.run(run())
```

---

## 6. Layer 1: Graph å®šä¹‰è§„èŒƒ

### 6.1 Graph å®šä¹‰ç¤ºä¾‹

```yaml
# agent-definitions/content-creation.yaml
apiVersion: agent/v1
kind: Graph
metadata:
  name: content-creation
  version: "1.0.0"
  description: "å†…å®¹åˆ›ä½œå·¥ä½œæµ"

spec:
  # è¾“å…¥å‚æ•°å®šä¹‰
  inputs:
    topic:
      type: string
      required: true
      description: "åˆ›ä½œä¸»é¢˜"
    platform:
      type: string
      required: true
      enum: [xiaohongshu, douyin, weibo, wechat_mp, bilibili]
    style:
      type: string
      default: "casual"
      enum: [casual, professional, humorous, educational]
    keywords:
      type: array
      items: string
      default: []

  # çŠ¶æ€å®šä¹‰
  state:
    stage:
      type: string
      initial: "init"
    research_results:
      type: array
    outline:
      type: string
    draft:
      type: string
    polished_content:
      type: string
    images:
      type: array
    final_content:
      type: string
    final_title:
      type: string
    error:
      type: string

  # èŠ‚ç‚¹å®šä¹‰
  nodes:
    - name: research
      tool: web_search
      params:
        query: "${inputs.topic} ${inputs.platform} çƒ­é—¨å†…å®¹ æœ€æ–°è¶‹åŠ¿"
        num_results: 10
      outputs:
        research_results: "$.results"

    - name: outline
      tool: llm_generate
      params:
        model: "${runtime.model.default}"
        system: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‡ªåª’ä½“å†…å®¹ç­–åˆ’å¸ˆ"
        prompt: |
          åŸºäºä»¥ä¸‹ç ”ç©¶èµ„æ–™ï¼Œä¸ºä¸»é¢˜ã€Œ${inputs.topic}ã€åˆ›å»ºå†…å®¹å¤§çº²ã€‚
          ç›®æ ‡å¹³å°: ${inputs.platform}
          å†™ä½œé£æ ¼: ${inputs.style}
          ç ”ç©¶èµ„æ–™: ${state.research_results}
      outputs:
        outline: "$.content"

    - name: draft
      tool: llm_generate
      params:
        model: "${runtime.model.default}"
        max_tokens: 8192
        prompt: |
          æ ¹æ®ä»¥ä¸‹å¤§çº²æ’°å†™å®Œæ•´å†…å®¹ã€‚
          å¹³å°: ${inputs.platform}
          å¤§çº²: ${state.outline}
      outputs:
        draft: "$.content"

    - name: polish
      tool: llm_generate
      params:
        model: "${runtime.model.default}"
        prompt: "æ¶¦è‰²ä»¥ä¸‹å†…å®¹: ${state.draft}"
      outputs:
        polished_content: "$.content"

    - name: generate_images
      tool: image_gen
      capability:
        required: false        # éå¿…éœ€å·¥å…·
        fallback: skip         # ä¸å¯ç”¨æ—¶è·³è¿‡
      params:
        prompt: "ä¸ºæ–‡ç« ç”Ÿæˆé…å›¾: ${state.polished_content[:200]}"
        count: 3
      outputs:
        images: "$.image_urls"

    - name: review
      tool: llm_generate
      params:
        model: "${runtime.model.default}"
        prompt: "å®¡æ ¸å†…å®¹è´¨é‡: ${state.polished_content}"
      outputs:
        review_result: "$.result"

  # è¾¹å®šä¹‰ï¼ˆå·¥ä½œæµï¼‰
  edges:
    - from: START
      to: research
    - from: research
      to: outline
    - from: outline
      to: draft
    - from: draft
      to: polish
    - from: polish
      to: generate_images
    - from: generate_images
      to: review
    - from: review
      to: END
      condition: "${state.review_result.passed == true}"
    - from: review
      to: polish
      condition: "${state.review_result.passed == false && state.revision_count < 3}"

  # è¾“å‡ºå®šä¹‰
  outputs:
    content: "${state.polished_content}"
    title: "${state.final_title}"
    images: "${state.images}"
```

### 6.2 Graph åŠ è½½å™¨

```python
# packages/agent-core/src/agent_core/graph/loader.py

from typing import Any
from pathlib import Path
import yaml
import json


class GraphLoader:
    """Graph å®šä¹‰åŠ è½½å™¨"""

    def __init__(self, definitions_path: str = "agent-definitions"):
        self.definitions_path = Path(definitions_path)
        self._cache: dict[str, dict] = {}

    def load(self, graph_name: str) -> dict:
        """åŠ è½½ Graph å®šä¹‰"""
        if graph_name in self._cache:
            return self._cache[graph_name]

        yaml_path = self.definitions_path / f"{graph_name}.yaml"
        json_path = self.definitions_path / f"{graph_name}.json"

        if yaml_path.exists():
            with open(yaml_path, 'r', encoding='utf-8') as f:
                definition = yaml.safe_load(f)
        elif json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                definition = json.load(f)
        else:
            raise FileNotFoundError(f"Graph definition not found: {graph_name}")

        self._validate(definition)
        self._cache[graph_name] = definition
        return definition

    def _validate(self, definition: dict):
        """éªŒè¯ Graph å®šä¹‰"""
        required_fields = ['apiVersion', 'kind', 'metadata', 'spec']
        for field in required_fields:
            if field not in definition:
                raise ValueError(f"Missing required field: {field}")

        if definition['kind'] != 'Graph':
            raise ValueError(f"Invalid kind: {definition['kind']}")

    def list_graphs(self) -> list[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ Graph"""
        graphs = []
        for path in self.definitions_path.glob("*.yaml"):
            graphs.append(path.stem)
        for path in self.definitions_path.glob("*.json"):
            if path.stem not in graphs:
                graphs.append(path.stem)
        return graphs
```

---

## 7. ç»Ÿä¸€èµ„æº URI æ–¹æ¡ˆ

### 7.1 èµ„æº URI è®¾è®¡

```yaml
URI æ ¼å¼:
  asset://{runtime}/{type}/{id}

ç¤ºä¾‹:
  - asset://local/image/abc123         # æœ¬åœ°å›¾ç‰‡
  - asset://cloud/image/abc123         # äº‘ç«¯å›¾ç‰‡
  - asset://local/credential/xiaohongshu_user1  # æœ¬åœ°å‡­è¯
  - asset://cloud/credential/xiaohongshu_user1  # äº‘ç«¯å‡­è¯
  - asset://local/temp/draft_001       # æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
  - asset://cloud/storage/user123/file # äº‘ç«¯ç”¨æˆ·å­˜å‚¨
```

### 7.2 èµ„æºè§£æå™¨

```python
# packages/agent-core/src/agent_core/resource/resolver.py

from abc import ABC, abstractmethod
from typing import Optional
from pathlib import Path
import uuid

from .uri import AssetURI


class AssetResolver(ABC):
    """èµ„æºè§£æå™¨åŸºç±»"""

    @abstractmethod
    async def resolve(self, uri: str) -> str:
        """å°† URI è§£æä¸ºå®é™…è·¯å¾„/URL"""
        pass

    @abstractmethod
    async def store(self, type: str, data: bytes, id: Optional[str] = None) -> str:
        """å­˜å‚¨èµ„æºå¹¶è¿”å› URI"""
        pass


class LocalAssetResolver(AssetResolver):
    """æœ¬åœ°èµ„æºè§£æå™¨"""

    def __init__(self, config: dict):
        self.base_path = Path(config.get('data_path', '~/.ai-creator/data')).expanduser()

    async def resolve(self, uri: str) -> str:
        asset = AssetURI.parse(uri)

        if asset.runtime == "cloud":
            # äº‘ç«¯èµ„æºéœ€è¦ä¸‹è½½åˆ°æœ¬åœ°
            return await self._download_cloud_asset(asset)

        return str(self.base_path / asset.type / asset.id)

    async def store(self, type: str, data: bytes, id: Optional[str] = None) -> str:
        if id is None:
            id = str(uuid.uuid4())

        dir_path = self.base_path / type
        dir_path.mkdir(parents=True, exist_ok=True)

        file_path = dir_path / id
        file_path.write_bytes(data)

        return f"asset://local/{type}/{id}"

    async def _download_cloud_asset(self, asset: AssetURI) -> str:
        """ä¸‹è½½äº‘ç«¯èµ„æºåˆ°æœ¬åœ°ç¼“å­˜"""
        # å®ç°äº‘ç«¯èµ„æºä¸‹è½½
        pass


class CloudAssetResolver(AssetResolver):
    """äº‘ç«¯èµ„æºè§£æå™¨"""

    def __init__(self, config: dict, user_id: str):
        self.config = config
        self.user_id = user_id
        self.s3_bucket = config.get('s3_bucket', 'ai-creator-assets')

    async def resolve(self, uri: str) -> str:
        asset = AssetURI.parse(uri)

        if asset.runtime == "local":
            raise ValueError("Local asset not available in cloud runtime")

        return await self._generate_presigned_url(asset)

    async def store(self, type: str, data: bytes, id: Optional[str] = None) -> str:
        import boto3

        if id is None:
            id = str(uuid.uuid4())

        s3_key = f"{self.user_id}/{type}/{id}"

        s3_client = boto3.client('s3')
        s3_client.put_object(
            Bucket=self.s3_bucket,
            Key=s3_key,
            Body=data,
        )

        return f"asset://cloud/{type}/{id}"

    async def _generate_presigned_url(self, asset: AssetURI) -> str:
        import boto3

        s3_key = f"{self.user_id}/{asset.type}/{asset.id}"

        s3_client = boto3.client('s3')
        url = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.s3_bucket, 'Key': s3_key},
            ExpiresIn=3600,
        )

        return url
```

---

## 8. éƒ¨ç½²ç­–ç•¥

### 8.1 å¼€å‘ç¯å¢ƒ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/ai-creator.git
cd ai-creator

# å®‰è£… uvï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# åˆå§‹åŒ– workspaceï¼ˆå®‰è£…æ‰€æœ‰ä¾èµ–ï¼‰
uv sync

# å¼€å‘ Sidecar
cd apps/sidecar
uv run python -m sidecar.main

# å¼€å‘äº‘ç«¯
cd services/cloud-backend
uv run uvicorn backend.app.main:app --reload
```

### 8.2 Sidecar æ‰“åŒ…ï¼ˆæ¡Œé¢ç«¯ï¼‰

```bash
# æ–¹å¼ 1: PyInstaller æ‰“åŒ…ä¸ºå•æ–‡ä»¶
cd apps/sidecar
uv run pyinstaller --onefile --name ai-creator-sidecar src/sidecar/main.py

# æ–¹å¼ 2: Nuitka ç¼–è¯‘ï¼ˆæ›´å¥½çš„æ€§èƒ½ï¼‰
uv run nuitka --standalone --onefile src/sidecar/main.py

# è¾“å‡º: dist/ai-creator-sidecar (å¯æ‰§è¡Œæ–‡ä»¶)
# æ”¾å…¥ Tauri çš„ sidecar ç›®å½•
```

### 8.3 äº‘ç«¯éƒ¨ç½²ï¼ˆDockerï¼‰

```dockerfile
# services/cloud-backend/Dockerfile

FROM python:3.11-slim

# å®‰è£… uv
RUN pip install uv

WORKDIR /app

# å¤åˆ¶ workspace é…ç½®
COPY pyproject.toml uv.lock ./
COPY packages/agent-core ./packages/agent-core
COPY services/cloud-backend ./services/cloud-backend
COPY agent-definitions ./agent-definitions

# å®‰è£…ä¾èµ–
RUN uv sync --frozen --package ai-creator-cloud

# è¿è¡Œ
CMD ["uv", "run", "uvicorn", "backend.app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 8.4 å‘å¸ƒåˆ° PyPIï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦å°† `agent-core` å‘å¸ƒä¸ºç‹¬ç«‹åŒ…ï¼š

```bash
# æ„å»º
cd packages/agent-core
uv build

# å‘å¸ƒåˆ° PyPI
uv publish --token $PYPI_TOKEN

# å…¶ä»–é¡¹ç›®å¯ä»¥ç›´æ¥å®‰è£…
pip install agent-core
```

---

## 9. å¼€å‘è§„èŒƒ

### 9.1 å·¥å…·èƒ½åŠ›å£°æ˜è§„èŒƒ

```python
# æ¯ä¸ªå·¥å…·å¿…é¡»å£°æ˜èƒ½åŠ›å’Œæ”¯æŒçš„è¿è¡Œæ—¶
@ToolRegistry.register("my_tool", RuntimeType.LOCAL)
class MyLocalTool(ToolInterface):
    metadata = ToolMetadata(
        name="my_tool",
        description="å·¥å…·æè¿°",
        capabilities=[ToolCapability.XXX],
        supported_runtimes=[RuntimeType.LOCAL],  # ä»…æœ¬åœ°
        fallback_tool="fallback_tool_name",       # é™çº§æ–¹æ¡ˆ
    )
```

### 9.2 è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ä½¿ç”¨è§„èŒƒ

```python
# âŒ é”™è¯¯åšæ³•ï¼šç¡¬ç¼–ç é…ç½®
class BadTool(ToolInterface):
    async def execute(self, **kwargs):
        api_key = os.environ.get("ANTHROPIC_API_KEY")  # ç›´æ¥è¯»ç¯å¢ƒå˜é‡

# âœ… æ­£ç¡®åšæ³•ï¼šä»ä¸Šä¸‹æ–‡è·å–
class GoodTool(ToolInterface):
    async def execute(self, ctx: RuntimeContext, **kwargs):
        api_key = ctx.api_keys.get("anthropic")  # ä»ä¸Šä¸‹æ–‡è·å–
```

### 9.3 èµ„æº URI ä½¿ç”¨è§„èŒƒ

```python
# âŒ ç¦æ­¢ç¡¬ç¼–ç è·¯å¾„
image_path = "/Users/mac/.ai-creator/images/abc.png"

# âœ… ä½¿ç”¨ç»Ÿä¸€ URI
image_uri = "asset://local/image/abc"
# æˆ–
image_uri = await ctx.asset_resolver.store("image", image_data)
```

---

## 10. LLM ç»Ÿä¸€æ¥å£è®¾è®¡

> æ›´æ–°: 2025-12-28 | äº‘ç«¯LLMç½‘å…³å·²å®Œæˆå¼€å‘ï¼Œæ¡Œé¢ç«¯é€šè¿‡ç»Ÿä¸€æ¥å£è°ƒç”¨

### 10.1 è®¾è®¡ç†å¿µ

**æ ¸å¿ƒåŸåˆ™**: agent-core æä¾›ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ï¼Œå±è”½ç«¯äº‘å®ç°å·®å¼‚ã€‚

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LLM ç»Ÿä¸€æ¥å£æ¶æ„                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    agent-core: LLMClientInterface                     â”‚  â”‚
â”‚  â”‚                         (ç»Ÿä¸€è°ƒç”¨æ¥å£)                                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  - chat(messages, model, ...)       â†’ LLMResponse              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - chat_stream(messages, model, ...)â†’ AsyncIterator[str]       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - get_available_models()           â†’ List[ModelInfo]          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚              â”‚                                               â”‚              â”‚
â”‚              â–¼                                               â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   CloudLLMClient        â”‚                 â”‚   DirectLLMClient       â”‚   â”‚
â”‚  â”‚   (æ¡Œé¢ç«¯/Sidecar)       â”‚                 â”‚   (äº‘ç«¯æœåŠ¡)             â”‚   â”‚
â”‚  â”‚                         â”‚                 â”‚                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ HTTP/HTTPS è°ƒç”¨   â”‚  â”‚                 â”‚  â”‚ ç›´æ¥SDKè°ƒç”¨       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ äº‘ç«¯ LLM ç½‘å…³     â”‚  â”‚                 â”‚  â”‚ LLMGateway å®ä¾‹   â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚               â”‚                                           â”‚                â”‚
â”‚               â”‚  API Token + baseUrl                      â”‚  å†…éƒ¨è°ƒç”¨       â”‚
â”‚               â–¼                                           â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      äº‘ç«¯ LLM ç½‘å…³ (å·²å®Œæˆ)                           â”‚   â”‚
â”‚  â”‚   - OpenAI å…¼å®¹æ¥å£: POST /v1/chat/completions                      â”‚   â”‚
â”‚  â”‚   - Anthropic å…¼å®¹æ¥å£: POST /v1/messages                           â”‚   â”‚
â”‚  â”‚   - å¤šä¾›åº”å•†æ•…éšœè½¬ç§»ã€ç†”æ–­å™¨ã€ç”¨é‡è¿½è¸ª                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 æ¥å£å®šä¹‰

```python
# packages/agent-core/src/agent_core/llm/interface.py

from abc import ABC, abstractmethod
from typing import Any, Optional, AsyncIterator, List
from dataclasses import dataclass, field
from enum import Enum


class LLMProvider(str, Enum):
    """LLM ä¾›åº”å•†"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    ALIBABA = "alibaba"
    DEEPSEEK = "deepseek"
    ZHIPU = "zhipu"


@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    model_id: str                    # gpt-4o, claude-3-5-sonnet
    provider: LLMProvider
    display_name: str
    max_tokens: int
    supports_streaming: bool = True
    supports_vision: bool = False
    supports_tools: bool = True


@dataclass
class LLMMessage:
    """LLM æ¶ˆæ¯"""
    role: str  # system, user, assistant
    content: str


@dataclass
class LLMUsage:
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    input_tokens: int
    output_tokens: int
    total_tokens: int


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    model: str
    provider: str
    usage: LLMUsage
    cost: float = 0.0
    latency_ms: int = 0
    finish_reason: str = "stop"


@dataclass
class LLMConfig:
    """LLM å®¢æˆ·ç«¯é…ç½®"""
    # ç½‘å…³é…ç½®
    base_url: str                    # LLM ç½‘å…³åœ°å€
    api_token: str                   # API Token (sk-cf-xxx)

    # ç¯å¢ƒé…ç½®
    environment: str = "production"  # development | production

    # é»˜è®¤å‚æ•°
    default_model: str = "claude-sonnet-4-20250514"
    default_max_tokens: int = 4096
    default_temperature: float = 0.7

    # è¶…æ—¶é…ç½®
    timeout_seconds: int = 120
    retry_count: int = 3

    # å¯é€‰: ç›´æ¥è°ƒç”¨é…ç½® (äº‘ç«¯ä½¿ç”¨)
    direct_mode: bool = False        # æ˜¯å¦ç›´æ¥è°ƒç”¨ (è·³è¿‡ HTTP)


class LLMClientInterface(ABC):
    """
    LLM å®¢æˆ·ç«¯ç»Ÿä¸€æ¥å£

    - ç«¯ä¾§å®ç°: CloudLLMClient (HTTP è°ƒç”¨äº‘ç«¯ç½‘å…³)
    - äº‘ç«¯å®ç°: DirectLLMClient (ç›´æ¥è°ƒç”¨ LLMGateway)
    """

    @abstractmethod
    async def chat(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> LLMResponse:
        """
        å‘é€å¯¹è¯è¯·æ±‚

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§° (å¯é€‰, ä½¿ç”¨é»˜è®¤æ¨¡å‹)
            system: ç³»ç»Ÿæç¤º (å¯é€‰)
            max_tokens: æœ€å¤§ç”Ÿæˆ tokens (å¯é€‰)
            temperature: æ¸©åº¦å‚æ•° (å¯é€‰)
            user_id: ç”¨æˆ·ID (ç”¨äºç”¨é‡è¿½è¸ª)

        Returns:
            LLMResponse: å“åº”ç»“æœ
        """
        pass

    @abstractmethod
    async def chat_stream(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """
        æµå¼å¯¹è¯è¯·æ±‚

        Yields:
            str: é€å­—è¿”å›çš„å†…å®¹ç‰‡æ®µ
        """
        pass

    @abstractmethod
    async def get_available_models(self) -> List[ModelInfo]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

        Returns:
            List[ModelInfo]: æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        pass

    @abstractmethod
    async def get_usage_summary(
        self,
        user_id: str,
        period: str = "month"
    ) -> dict:
        """
        è·å–ç”¨é‡æ±‡æ€»

        Args:
            user_id: ç”¨æˆ·ID
            period: ç»Ÿè®¡å‘¨æœŸ (day, week, month)

        Returns:
            dict: ç”¨é‡ç»Ÿè®¡æ•°æ®
        """
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """
        å¥åº·æ£€æŸ¥

        Returns:
            bool: ç½‘å…³æ˜¯å¦å¯ç”¨
        """
        pass
```

### 10.3 æ¡Œé¢ç«¯å®ç° (CloudLLMClient)

```python
# packages/agent-core/src/agent_core/llm/cloud_client.py

import aiohttp
import json
from typing import List, Optional, AsyncIterator

from .interface import (
    LLMClientInterface, LLMConfig, LLMMessage,
    LLMResponse, LLMUsage, ModelInfo
)


class CloudLLMClient(LLMClientInterface):
    """
    äº‘ç«¯ LLM å®¢æˆ·ç«¯ - æ¡Œé¢ç«¯/Sidecar ä½¿ç”¨

    é€šè¿‡ HTTP è°ƒç”¨äº‘ç«¯ LLM ç½‘å…³ï¼Œæ”¯æŒ OpenAI å…¼å®¹æ ¼å¼ã€‚
    """

    def __init__(self, config: LLMConfig):
        self.config = config
        self._session: Optional[aiohttp.ClientSession] = None

    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å– HTTP ä¼šè¯"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                headers={
                    "Authorization": f"Bearer {self.config.api_token}",
                    "Content-Type": "application/json",
                },
                timeout=aiohttp.ClientTimeout(total=self.config.timeout_seconds),
            )
        return self._session

    async def chat(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> LLMResponse:
        """å‘é€å¯¹è¯è¯·æ±‚ (OpenAI å…¼å®¹æ ¼å¼)"""
        session = await self._get_session()

        # æ„å»ºæ¶ˆæ¯
        api_messages = []
        if system:
            api_messages.append({"role": "system", "content": system})
        for msg in messages:
            api_messages.append({"role": msg.role, "content": msg.content})

        # æ„å»ºè¯·æ±‚ä½“
        payload = {
            "model": model or self.config.default_model,
            "messages": api_messages,
            "max_tokens": max_tokens or self.config.default_max_tokens,
            "temperature": temperature or self.config.default_temperature,
            "stream": False,
        }
        if user_id:
            payload["user"] = user_id

        # å‘é€è¯·æ±‚
        url = f"{self.config.base_url}/v1/chat/completions"

        async with session.post(url, json=payload) as response:
            if response.status != 200:
                error = await response.text()
                raise LLMError(f"LLM request failed: {response.status} - {error}")

            data = await response.json()

        # è§£æå“åº”
        choice = data["choices"][0]
        usage = data.get("usage", {})

        return LLMResponse(
            content=choice["message"]["content"],
            model=data["model"],
            provider="cloud",
            usage=LLMUsage(
                input_tokens=usage.get("prompt_tokens", 0),
                output_tokens=usage.get("completion_tokens", 0),
                total_tokens=usage.get("total_tokens", 0),
            ),
            finish_reason=choice.get("finish_reason", "stop"),
        )

    async def chat_stream(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """æµå¼å¯¹è¯è¯·æ±‚"""
        session = await self._get_session()

        api_messages = []
        if system:
            api_messages.append({"role": "system", "content": system})
        for msg in messages:
            api_messages.append({"role": msg.role, "content": msg.content})

        payload = {
            "model": model or self.config.default_model,
            "messages": api_messages,
            "max_tokens": max_tokens or self.config.default_max_tokens,
            "temperature": temperature or self.config.default_temperature,
            "stream": True,
        }

        url = f"{self.config.base_url}/v1/chat/completions"

        async with session.post(url, json=payload) as response:
            if response.status != 200:
                error = await response.text()
                raise LLMError(f"LLM stream request failed: {response.status}")

            async for line in response.content:
                line = line.decode('utf-8').strip()
                if not line or line == "data: [DONE]":
                    continue
                if line.startswith("data: "):
                    try:
                        data = json.loads(line[6:])
                        delta = data["choices"][0].get("delta", {})
                        if "content" in delta:
                            yield delta["content"]
                    except json.JSONDecodeError:
                        continue

    async def get_available_models(self) -> List[ModelInfo]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        session = await self._get_session()
        url = f"{self.config.base_url}/api/v1/llm/models"

        async with session.get(url) as response:
            if response.status != 200:
                return []
            data = await response.json()

        return [
            ModelInfo(
                model_id=m["model_id"],
                provider=m["provider"],
                display_name=m["display_name"],
                max_tokens=m.get("max_tokens", 4096),
                supports_streaming=m.get("supports_streaming", True),
                supports_vision=m.get("supports_vision", False),
            )
            for m in data.get("models", [])
        ]

    async def get_usage_summary(
        self,
        user_id: str,
        period: str = "month"
    ) -> dict:
        """è·å–ç”¨é‡æ±‡æ€»"""
        session = await self._get_session()
        url = f"{self.config.base_url}/api/v1/llm/usage/summary"
        params = {"period": period}

        async with session.get(url, params=params) as response:
            if response.status != 200:
                return {}
            return await response.json()

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            session = await self._get_session()
            url = f"{self.config.base_url}/health"
            async with session.get(url) as response:
                return response.status == 200
        except Exception:
            return False

    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self._session and not self._session.closed:
            await self._session.close()


class LLMError(Exception):
    """LLM è°ƒç”¨å¼‚å¸¸"""
    pass
```

### 10.4 äº‘ç«¯å®ç° (DirectLLMClient)

```python
# packages/agent-core/src/agent_core/llm/direct_client.py

from typing import List, Optional, AsyncIterator

from .interface import (
    LLMClientInterface, LLMConfig, LLMMessage,
    LLMResponse, LLMUsage, ModelInfo
)


class DirectLLMClient(LLMClientInterface):
    """
    ç›´æ¥è°ƒç”¨ LLM å®¢æˆ·ç«¯ - äº‘ç«¯æœåŠ¡ä½¿ç”¨

    ç›´æ¥è°ƒç”¨ LLMGateway å®ä¾‹ï¼Œæ— éœ€ HTTP å¼€é”€ã€‚
    """

    def __init__(self, gateway: "LLMGateway", config: LLMConfig):
        """
        Args:
            gateway: LLMGateway å®ä¾‹ (äº‘ç«¯æœåŠ¡æ³¨å…¥)
            config: LLM é…ç½®
        """
        self.gateway = gateway
        self.config = config

    async def chat(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> LLMResponse:
        """ç›´æ¥è°ƒç”¨ LLMGateway"""
        api_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]

        response = await self.gateway.chat(
            messages=api_messages,
            model_id=model or self.config.default_model,
            user_id=user_id,
            system=system,
            max_tokens=max_tokens or self.config.default_max_tokens,
            temperature=temperature or self.config.default_temperature,
            stream=False,
        )

        return LLMResponse(
            content=response.content,
            model=response.model,
            provider=response.provider,
            usage=LLMUsage(
                input_tokens=response.usage["input_tokens"],
                output_tokens=response.usage["output_tokens"],
                total_tokens=response.usage["input_tokens"] + response.usage["output_tokens"],
            ),
            cost=response.cost,
            latency_ms=response.latency_ms,
        )

    async def chat_stream(
        self,
        messages: List[LLMMessage],
        *,
        model: Optional[str] = None,
        system: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        user_id: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """ç›´æ¥è°ƒç”¨ LLMGateway æµå¼æ¥å£"""
        api_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]

        async for chunk in self.gateway.chat_stream(
            messages=api_messages,
            model_id=model or self.config.default_model,
            user_id=user_id,
            system=system,
            max_tokens=max_tokens or self.config.default_max_tokens,
            temperature=temperature or self.config.default_temperature,
        ):
            yield chunk

    async def get_available_models(self) -> List[ModelInfo]:
        """ä»æ•°æ®åº“è·å–æ¨¡å‹åˆ—è¡¨"""
        models = await self.gateway.get_available_models()
        return [
            ModelInfo(
                model_id=m.model_id,
                provider=m.provider,
                display_name=m.display_name,
                max_tokens=m.max_tokens,
                supports_streaming=m.supports_streaming,
                supports_vision=m.supports_vision,
            )
            for m in models
        ]

    async def get_usage_summary(
        self,
        user_id: str,
        period: str = "month"
    ) -> dict:
        """ä»ç”¨é‡è¿½è¸ªå™¨è·å–ç»Ÿè®¡"""
        return await self.gateway.usage_tracker.get_usage_summary(
            user_id, period
        )

    async def health_check(self) -> bool:
        """æ£€æŸ¥ç½‘å…³çŠ¶æ€"""
        return True  # ç›´æ¥è°ƒç”¨ï¼Œå§‹ç»ˆå¯ç”¨
```

### 10.5 é…ç½®ç®¡ç†

> æ›´æ–°: 2025-12-28 | æ¡Œé¢ç«¯ç”¨æˆ·ç™»å½•åè‡ªåŠ¨è·å– API Tokenï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®

```python
# packages/agent-core/src/agent_core/llm/config.py

import os
import json
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict

from .interface import LLMConfig


class LLMConfigManager:
    """
    LLM é…ç½®ç®¡ç†å™¨

    - æ¡Œé¢ç«¯: ç”¨æˆ·ç™»å½•åè‡ªåŠ¨è·å–å¹¶ä¿å­˜ API Token
    - æ— éœ€æ‰‹åŠ¨é…ç½®ï¼ŒToken ç”±æœåŠ¡ç«¯åˆ†å‘
    - æ”¯æŒå¼€å‘/ç”Ÿäº§ç¯å¢ƒåˆ‡æ¢ (ä»…å¼€å‘è€…ä½¿ç”¨)
    """

    # é»˜è®¤é…ç½®è·¯å¾„
    DEFAULT_CONFIG_PATH = "~/.ai-creator/llm-config.json"

    # é»˜è®¤ç½‘å…³åœ°å€ (å›ºå®šï¼Œç”¨æˆ·æ— éœ€é…ç½®)
    DEFAULT_URLS = {
        "development": "http://localhost:8001",
        "production": "https://api.ai-creator.com",
    }

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = Path(
            config_path or os.path.expanduser(self.DEFAULT_CONFIG_PATH)
        )
        self._config: Optional[LLMConfig] = None

    def load(self, environment: str = "production") -> LLMConfig:
        """
        åŠ è½½é…ç½®

        Args:
            environment: ç¯å¢ƒåç§° (development | production)

        Returns:
            LLMConfig: LLM é…ç½®
        """
        # ä»é…ç½®æ–‡ä»¶è¯»å– (è‡ªåŠ¨ä¿å­˜çš„ Token)
        if self.config_path.exists():
            with open(self.config_path, 'r') as f:
                data = json.load(f)

            env_config = data.get(environment, {})
            return LLMConfig(
                base_url=self.DEFAULT_URLS[environment],  # å›ºå®šåœ°å€
                api_token=env_config.get("api_token", ""),
                environment=environment,
                default_model=env_config.get("default_model", "claude-sonnet-4-20250514"),
                timeout_seconds=env_config.get("timeout_seconds", 120),
            )

        # è¿”å›é»˜è®¤é…ç½® (æ—  Tokenï¼Œéœ€è¦ç™»å½•)
        return LLMConfig(
            base_url=self.DEFAULT_URLS[environment],
            api_token="",
            environment=environment,
        )

    def save_token(self, api_token: str, environment: str = "production"):
        """
        ä¿å­˜ API Token (ç”¨æˆ·ç™»å½•åè‡ªåŠ¨è°ƒç”¨)

        Args:
            api_token: æœåŠ¡ç«¯åˆ†å‘çš„ API Token (sk-cf-xxx)
            environment: ç¯å¢ƒåç§°
        """
        self.config_path.parent.mkdir(parents=True, exist_ok=True)

        # è¯»å–ç°æœ‰é…ç½®
        data = {}
        if self.config_path.exists():
            with open(self.config_path, 'r') as f:
                data = json.load(f)

        # æ›´æ–° Token
        if environment not in data:
            data[environment] = {}
        data[environment]["api_token"] = api_token

        # å†™å…¥é…ç½®
        with open(self.config_path, 'w') as f:
            json.dump(data, f, indent=2)

        # è®¾ç½®æ–‡ä»¶æƒé™ (ä»…æ‰€æœ‰è€…å¯è¯»å†™)
        self.config_path.chmod(0o600)

    def clear_token(self, environment: str = "production"):
        """
        æ¸…é™¤ Token (ç”¨æˆ·ç™»å‡ºæ—¶è°ƒç”¨)
        """
        if not self.config_path.exists():
            return

        with open(self.config_path, 'r') as f:
            data = json.load(f)

        if environment in data:
            data[environment]["api_token"] = ""

        with open(self.config_path, 'w') as f:
            json.dump(data, f, indent=2)

    def is_logged_in(self, environment: str = "production") -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç™»å½• (æ˜¯å¦æœ‰ Token)"""
        config = self.load(environment)
        return bool(config.api_token)

    def get_current_environment(self) -> str:
        """è·å–å½“å‰ç¯å¢ƒ"""
        return os.environ.get("AI_CREATOR_ENV", "production")
```

### 10.6 æ¡Œé¢ç«¯é…ç½®æ–‡ä»¶ç¤ºä¾‹

> ç”¨æˆ·ç™»å½•åè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨åˆ›å»º

```json
// ~/.ai-creator/llm-config.json (è‡ªåŠ¨ç”Ÿæˆ)
{
  "production": {
    "api_token": "sk-cf-xxxxxxxxxxxxxx"
  }
}
```

### 10.7 ç™»å½•æµç¨‹é›†æˆ

```python
# apps/sidecar/src/sidecar/auth.py

from agent_core.llm.config import LLMConfigManager


async def on_user_login(user_token: str, api_base_url: str):
    """
    ç”¨æˆ·ç™»å½•æˆåŠŸåçš„å›è°ƒ

    Args:
        user_token: ç”¨æˆ·ç™»å½• Token (JWT)
        api_base_url: äº‘ç«¯ API åœ°å€
    """
    import aiohttp

    # 1. è°ƒç”¨äº‘ç«¯æ¥å£è·å– LLM API Token
    async with aiohttp.ClientSession() as session:
        async with session.post(
            f"{api_base_url}/api/v1/auth/llm-token",
            headers={"Authorization": f"Bearer {user_token}"},
        ) as response:
            if response.status == 200:
                data = await response.json()
                api_token = data["api_token"]

                # 2. è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°é…ç½®
                config_manager = LLMConfigManager()
                config_manager.save_token(api_token)

                return True

    return False


async def on_user_logout():
    """ç”¨æˆ·ç™»å‡ºæ—¶æ¸…é™¤ Token"""
    config_manager = LLMConfigManager()
    config_manager.clear_token()
```

æ›´æ–°åçš„ LLM å·¥å…·ä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼š

```python
# packages/agent-core/src/agent_core/tools/builtin/llm.py

from ..base import ToolInterface, ToolMetadata, ToolCapability, ToolResult
from ..registry import ToolRegistry
from ...runtime.interfaces import RuntimeContext
from ...llm.interface import LLMMessage


@ToolRegistry.register_universal("llm_generate")
class LLMGenerateTool(ToolInterface):
    """
    LLM æ–‡æœ¬ç”Ÿæˆå·¥å…· - ç«¯äº‘ç»Ÿä¸€

    è‡ªåŠ¨æ ¹æ®è¿è¡Œæ—¶ç¯å¢ƒé€‰æ‹©:
    - æ¡Œé¢ç«¯: ä½¿ç”¨ CloudLLMClient (HTTP è°ƒç”¨äº‘ç«¯ç½‘å…³)
    - äº‘ç«¯: ä½¿ç”¨ DirectLLMClient (ç›´æ¥è°ƒç”¨ LLMGateway)
    """

    metadata = ToolMetadata(
        name="llm_generate",
        description="ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬",
        capabilities=[ToolCapability.LLM_GENERATE],
    )

    async def execute(
        self,
        ctx: RuntimeContext,
        *,
        prompt: str,
        system: str = "",
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
    ) -> ToolResult:
        """æ‰§è¡Œ LLM ç”Ÿæˆ"""

        # ä»è¿è¡Œæ—¶ä¸Šä¸‹æ–‡è·å– LLM å®¢æˆ·ç«¯
        llm_client = ctx.extra.get("llm_client")
        if not llm_client:
            return ToolResult(
                success=False,
                data=None,
                error="LLM client not configured"
            )

        try:
            messages = [LLMMessage(role="user", content=prompt)]

            response = await llm_client.chat(
                messages=messages,
                model=model or ctx.model_default,
                system=system,
                max_tokens=max_tokens,
                temperature=temperature,
                user_id=ctx.user_id,
            )

            return ToolResult(
                success=True,
                data={
                    "content": response.content,
                    "model": response.model,
                    "usage": {
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens,
                    },
                    "cost": response.cost,
                }
            )

        except Exception as e:
            return ToolResult(success=False, data=None, error=str(e))

    def get_schema(self) -> dict:
        return {
            "type": "object",
            "properties": {
                "prompt": {"type": "string", "description": "ç”¨æˆ·æç¤º"},
                "system": {"type": "string", "description": "ç³»ç»Ÿæç¤º"},
                "model": {"type": "string", "description": "æ¨¡å‹åç§°"},
                "max_tokens": {"type": "integer", "default": 4096},
                "temperature": {"type": "number", "default": 0.7},
            },
            "required": ["prompt"]
        }
```

### 10.8 è¿è¡Œæ—¶ä¸Šä¸‹æ–‡æ›´æ–°

```python
# packages/agent-core/src/agent_core/runtime/interfaces.py (æ›´æ–°)

@dataclass
class RuntimeContext:
    """è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ - ç»Ÿä¸€ç«¯äº‘é…ç½®æ³¨å…¥"""
    runtime_type: RuntimeType
    user_id: str
    inputs: dict

    # æ¨¡å‹é…ç½®
    model_default: str = "claude-sonnet-4-20250514"
    model_fast: str = "claude-3-5-haiku-20241022"

    # ğŸ”¥ LLM å®¢æˆ·ç«¯ (æ–°å¢)
    # ç”±ç«¯/äº‘å®ç°æ³¨å…¥ï¼Œç»Ÿä¸€æ¥å£
    llm_client: Optional["LLMClientInterface"] = None

    # èµ„æºè§£æå™¨ï¼ˆç”±ç«¯/äº‘å®ç°æ³¨å…¥ï¼‰
    asset_resolver: Optional["AssetResolver"] = None

    # é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆç«¯/äº‘ç‰¹æœ‰ï¼‰
    extra: dict = field(default_factory=dict)
```

### 10.9 ç«¯ä¾§æ‰§è¡Œå™¨åˆå§‹åŒ–ç¤ºä¾‹

```python
# apps/sidecar/src/sidecar/executor.py (æ›´æ–°)

from agent_core.llm.cloud_client import CloudLLMClient
from agent_core.llm.config import LLMConfigManager


class LocalExecutor(ExecutorInterface):
    """æœ¬åœ°æ‰§è¡Œå™¨ - æ¡Œé¢ç«¯ Python Sidecar"""

    def __init__(self, config: dict):
        self.config = config

        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        llm_config_manager = LLMConfigManager()
        llm_config = llm_config_manager.load(
            environment=config.get('environment', 'production')
        )
        self.llm_client = CloudLLMClient(llm_config)

        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...

    def _create_context(self, request: ExecutionRequest) -> RuntimeContext:
        """åˆ›å»ºè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
        return RuntimeContext(
            runtime_type=RuntimeType.LOCAL,
            user_id=request.user_id,
            inputs=request.inputs,
            model_default=self.config.get('default_model', 'claude-sonnet-4-20250514'),
            model_fast=self.config.get('fast_model', 'claude-3-5-haiku-20241022'),
            llm_client=self.llm_client,  # ğŸ”¥ æ³¨å…¥ LLM å®¢æˆ·ç«¯
            asset_resolver=LocalAssetResolver(self.config),
            extra={
                "browser_manager": self._get_browser_manager(),
            }
        )
```

### 10.10 äº‘ç«¯æ‰§è¡Œå™¨åˆå§‹åŒ–ç¤ºä¾‹

```python
# services/cloud-backend/backend/app/agent/executor.py (æ›´æ–°)

from agent_core.llm.direct_client import DirectLLMClient
from agent_core.llm.interface import LLMConfig


class CloudExecutor(ExecutorInterface):
    """äº‘ç«¯æ‰§è¡Œå™¨ - FastAPI åç«¯"""

    def __init__(self, db, redis, config: dict, browser_pool=None, llm_gateway=None):
        self.llm_gateway = llm_gateway

        # åˆå§‹åŒ–ç›´æ¥è°ƒç”¨å®¢æˆ·ç«¯
        llm_config = LLMConfig(
            base_url="",  # ç›´æ¥è°ƒç”¨ä¸éœ€è¦
            api_token="",
            direct_mode=True,
        )
        self.llm_client = DirectLLMClient(llm_gateway, llm_config)

        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...

    def _create_context(self, request: ExecutionRequest) -> RuntimeContext:
        """åˆ›å»ºè¿è¡Œæ—¶ä¸Šä¸‹æ–‡"""
        return RuntimeContext(
            runtime_type=RuntimeType.CLOUD,
            user_id=request.user_id,
            inputs=request.inputs,
            model_default=self._get_user_model(request.user_id),
            model_fast='claude-3-5-haiku-20241022',
            llm_client=self.llm_client,  # ğŸ”¥ æ³¨å…¥ LLM å®¢æˆ·ç«¯
            asset_resolver=CloudAssetResolver(self.config, request.user_id),
            extra={
                "db": self.db,
                "redis": self.redis,
                "browser_pool": self.browser_pool,
            }
        )
```

### 10.11 agent-core åŒ…ç»“æ„æ›´æ–°

```text
packages/agent-core/src/agent_core/
â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ llm/                                # ğŸ”¥ æ–°å¢: LLM ç»Ÿä¸€æ¥å£å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ interface.py                    # LLMClientInterface æŠ½è±¡æ¥å£
â”‚   â”œâ”€â”€ cloud_client.py                 # CloudLLMClient (HTTP è°ƒç”¨)
â”‚   â”œâ”€â”€ direct_client.py                # DirectLLMClient (ç›´æ¥è°ƒç”¨)
â”‚   â””â”€â”€ config.py                       # LLMConfigManager é…ç½®ç®¡ç†
â”‚
â”œâ”€â”€ graph/                              # Graph å®šä¹‰å±‚
â”‚   â””â”€â”€ ...
â”œâ”€â”€ runtime/                            # è¿è¡Œæ—¶å±‚
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tools/                              # å·¥å…·å±‚
â”‚   â””â”€â”€ ...
â”œâ”€â”€ resource/                           # èµ„æºç®¡ç†
â”‚   â””â”€â”€ ...
â””â”€â”€ platforms/                          # å¹³å°é€‚é…å™¨
    â””â”€â”€ ...
```

---

## 11. æ€»ç»“

### 11.1 æ¶æ„ä¼˜åŠ¿

| ç‰¹æ€§ | å®ç°æ–¹å¼ |
|------|---------|
| **ä»£ç å…±äº«** | `agent-core` åŒ…ï¼Œuv workspace ç®¡ç† |
| **ç‰ˆæœ¬åŒæ­¥** | Monorepo å•ä¸€ä»“åº“ï¼Œè‡ªåŠ¨åŒæ­¥ |
| **ç«¯äº‘å¯¹ç­‰** | ç»Ÿä¸€æ¥å£ï¼Œä¸åŒå®ç° |
| **å·¥å…·éš”ç¦»** | ToolRegistry æŒ‰è¿è¡Œæ—¶æ³¨å†Œ |
| **èµ„æºç»Ÿä¸€** | AssetURI ç»Ÿä¸€èµ„æºæ ‡è¯† |
| **éƒ¨ç½²çµæ´»** | PyInstaller æ‰“åŒ… / Docker éƒ¨ç½² / PyPI å‘å¸ƒ |

### 11.2 å¼€å‘æµç¨‹

```text
1. ä¿®æ”¹ agent-core â†’ ç«¯ä¾§/äº‘ç«¯è‡ªåŠ¨ç”Ÿæ•ˆ
2. å¼€å‘ç«¯ä¾§å·¥å…· â†’ æ³¨å†Œåˆ° RuntimeType.LOCAL
3. å¼€å‘äº‘ç«¯å·¥å…· â†’ æ³¨å†Œåˆ° RuntimeType.CLOUD
4. å®šä¹‰ Graph â†’ è‡ªåŠ¨åœ¨ç«¯ä¾§/äº‘ç«¯è¿è¡Œ
5. æ‰“åŒ…å‘å¸ƒ â†’ Sidecar äºŒè¿›åˆ¶ / Docker é•œåƒ
```

---

## ç›¸å…³æ–‡æ¡£

- [ç³»ç»Ÿæ¶æ„](./01-ç³»ç»Ÿæ¶æ„.md)
- [æ¡Œé¢ç«¯è®¾è®¡](./02-æ¡Œé¢ç«¯è®¾è®¡.md)
- [äº‘ç«¯æœåŠ¡è®¾è®¡](./04-äº‘ç«¯æœåŠ¡è®¾è®¡.md)
- [å¹³å°é€‚é…å™¨](./06-å¹³å°é€‚é…å™¨.md)
- [AIå·¥ä½œæµ](./07-AIå·¥ä½œæµ.md)
- [BettaFishèˆ†æƒ…åˆ†æé›†æˆ](./08-BettaFishèˆ†æƒ…åˆ†æé›†æˆ.md)
- [å¼€å‘è§„èŒƒ](./11-å¼€å‘è§„èŒƒ.md)
